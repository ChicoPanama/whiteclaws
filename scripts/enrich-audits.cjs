#!/usr/bin/env node
'use strict'

/*
  Enriches WhiteClaws protocols with audit references from:
  - data/audit_firm_index.json (generated by scripts/index-audit-firms.cjs)
  - data/merged_enrichment.json + data/website_contacts.json (already-scraped protocol links)
  - Optional live scanning:
    - GitHub repo tree scan for PDF files under common audit/security paths
    - Protocol website /security,/audits pages for PDF/audit links

  Output: data/audit_enrichment.json
  Checkpoint: data/.audit_checkpoint.json
*/

const fs = require('fs')
const path = require('path')

const USER_AGENT =
  'WhiteClaws-SecurityBot/1.0 (security research; contact: security@whiteclaws.xyz)'

const GITHUB_API = 'https://api.github.com'
const GITHUB_TOKEN = process.env.GITHUB_TOKEN || process.env.GITHUB_PAT || ''

const OUT_PATH = path.join('data', 'audit_enrichment.json')
const CHECKPOINT_PATH = path.join('data', '.audit_checkpoint.json')

const INPUT_PROTOCOL_DIR = path.join('public', 'protocols')
const MERGED_ENRICHMENT_PATH = path.join('data', 'merged_enrichment.json')
const WEBSITE_CONTACTS_PATH = path.join('data', 'website_contacts.json')
const FIRM_INDEX_PATH = path.join('data', 'audit_firm_index.json')

const DELAYS = {
  github_api: 2000,
  website_fetch: 3000,
  same_domain: 5000,
}

const AUDIT_DIR_PREFIXES = [
  'audits/',
  'audit/',
  'security/',
  'audit-reports/',
  'security-audits/',
  'docs/audits/',
  'docs/security/',
  'reports/',
]

const ALIASES = {
  '0x': ['0x', 'zeroex', 'zero-ex', '0x-protocol'],
  '88mphv3': ['88mph', '88-mph'],
  aave: ['aave', 'aave-v2', 'aave-v3', 'aave-v3.1'],
  compound: ['compound', 'compound-v2', 'compound-v3', 'compound-iii'],
  curve: ['curve', 'curve-dao', 'curve-fi'],
  lido: ['lido', 'lido-finance', 'steth'],
  maker: ['maker', 'makerdao', 'dai'],
  scroll: ['scroll', 'scroll-tech'],
  uniswap: ['uniswap', 'uniswap-v2', 'uniswap-v3', 'uniswap-v4'],
  yearn: ['yearn', 'yearn-finance', 'yvault'],
  sushi: ['sushi', 'sushiswap'],
  gmx: ['gmx', 'gmx-io'],
  morpho: ['morpho', 'morpho-labs', 'morpho-blue'],
  euler: ['euler', 'euler-finance'],
  frax: ['frax', 'frax-finance', 'fraxlend'],
  pendle: ['pendle', 'pendle-finance'],
  eigenlayer: ['eigenlayer', 'eigen-layer', 'eigen'],
  layerzero: ['layerzero', 'layer-zero'],
  chainlink: ['chainlink'],
  openzeppelin: ['openzeppelin', 'oz'],
}

function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms))
}

async function fetchWithTimeout(url, { headers = {}, timeoutMs = 15000 } = {}) {
  const controller = new AbortController()
  const timer = setTimeout(() => controller.abort(), timeoutMs)
  try {
    return await fetch(url, { headers, signal: controller.signal })
  } finally {
    clearTimeout(timer)
  }
}

let lastGithubAt = 0
async function githubFetch(url, { accept } = {}) {
  const now = Date.now()
  const wait = Math.max(0, lastGithubAt + DELAYS.github_api - now)
  if (wait) await sleep(wait)
  lastGithubAt = Date.now()

  const headers = {
    'User-Agent': USER_AGENT,
    Accept: accept || 'application/vnd.github.v3+json',
  }
  if (GITHUB_TOKEN) headers.Authorization = `token ${GITHUB_TOKEN}`

  return fetchWithTimeout(url, { headers, timeoutMs: 15000 })
}

const lastDomainAt = new Map()
async function politeWebsiteFetch(url) {
  let host = ''
  try {
    host = new URL(url).host
  } catch {
    // ignore
  }
  const now = Date.now()
  const last = lastDomainAt.get(host) || 0
  const wait = Math.max(0, last + DELAYS.same_domain - now, DELAYS.website_fetch)
  if (wait) await sleep(wait)
  lastDomainAt.set(host, Date.now())
  const headers = {
    'User-Agent': USER_AGENT,
    Accept: 'text/html,application/xhtml+xml,*/*',
  }
  return fetchWithTimeout(url, { headers, timeoutMs: 15000 })
}

function readJson(p, fallback) {
  try {
    return JSON.parse(fs.readFileSync(p, 'utf8'))
  } catch {
    return fallback
  }
}

function writeJson(p, obj) {
  fs.mkdirSync(path.dirname(p), { recursive: true })
  fs.writeFileSync(p, JSON.stringify(obj, null, 2))
}

function parseArgs(argv) {
  const out = { resume: false, limit: 0, start: 0, scanGithub: false, scanWebsite: false }
  for (let i = 0; i < argv.length; i++) {
    const a = argv[i]
    if (a === '--resume') out.resume = true
    else if (a === '--deep') out.scanGithub = out.scanWebsite = true
    else if (a === '--scan-github') out.scanGithub = true
    else if (a === '--scan-website') out.scanWebsite = true
    else if (a === '--fast') out.scanGithub = out.scanWebsite = false
    else if (a === '--limit') out.limit = Number(argv[++i] || 0)
    else if (a === '--start') out.start = Number(argv[++i] || 0)
  }
  return out
}

function norm(s) {
  return String(s || '')
    .toLowerCase()
    .replace(/&/g, ' and ')
    .replace(/[^a-z0-9]+/g, ' ')
    .replace(/\s+/g, ' ')
    .trim()
}

function stripCommonSuffixes(s) {
  const t = norm(s)
  return t
    .replace(/\b(protocol|finance|network|labs|dao|io|xyz|foundation|token|app|project)\b/g, ' ')
    .replace(/\s+/g, ' ')
    .trim()
}

function extractDate(s) {
  const str = String(s || '')
  const m = str.match(/\b(20\d{2})[-_/](\d{2})(?:[-_/](\d{2}))?\b/)
  if (m) return `${m[1]}-${m[2]}`
  const m2 = str.match(/\b(20\d{2})\b/)
  if (m2) return m2[1]
  return ''
}

function escSingleQuotes(s) {
  return String(s || '').replace(/'/g, "\\'")
}

function safeIdPart(s) {
  return norm(s).replace(/\s+/g, '-').replace(/[^a-z0-9\-]/g, '').slice(0, 60)
}

function classifyPrimitive(protocol, auditTitle) {
  const t = (auditTitle || '').toLowerCase()
  const cat = (protocol.category || '').toLowerCase()

  if (/lending|borrow|collateral|liquidat|atoken|ctoken/i.test(t)) return 'lending'
  if (/amm|swap|pool|liquidity|router/i.test(t)) return 'amm'
  if (/bridge|cross chain|cross-chain|relay|message|gateway/i.test(t)) return 'bridge'
  if (/vault|yield|strategy|harvest|earn/i.test(t)) return 'vault'
  if (/stak|validator|deposit|withdrawal|beacon/i.test(t)) return 'staking'
  if (/govern|vot|proposal|timelock/i.test(t)) return 'governance'
  if (/oracle|price\\.feed|data\\.feed|pyth|chainlink/i.test(t)) return 'oracle'
  if (/rollup|l2|sequencer|prover|zk/i.test(t)) return 'l2'
  if (/library|contracts|utils|openzeppelin|solmate/i.test(t)) return 'library'
  if (/perpetual|futures|option|synth|deriv/i.test(t)) return 'derivatives'
  if (/nft|erc\\-721|erc721|marketplace|collect/i.test(t)) return 'nft'
  if (/dex|exchange|order\\.book|trading|clob/i.test(t)) return 'dex'
  if (/token|erc\\-20|erc20|mint|burn|stable/i.test(t)) return 'token'
  if (/insur|cover|claim|underwrit/i.test(t)) return 'insurance'
  if (/stream|payment|vest|sablier|superfluid/i.test(t)) return 'payments'
  if (/privacy|zero\\.knowledge|mixer|tornado|aztec/i.test(t)) return 'privacy'

  if (cat === 'defi') return 'defi'
  if (cat === 'bridge') return 'bridge'
  if (cat === 'l1/l2') return 'l2'
  if (cat === 'dex') return 'dex'
  if (cat === 'infrastructure') return 'infrastructure'
  if (cat === 'governance') return 'governance'
  return 'other'
}

function parseGithubOwnerRepo(url) {
  const s = String(url || '')
  const m = s.match(/github\.com\/([^/]+)\/([^/#?]+)(?:[/?#]|$)/i)
  if (!m) return null
  return { owner: m[1], repo: m[2].replace(/\.git$/i, '') }
}

function toRawGithubUrl(url) {
  // https://github.com/o/r/blob/ref/path -> https://raw.githubusercontent.com/o/r/ref/path
  const m = String(url || '').match(/^https?:\/\/github\.com\/([^/]+)\/([^/]+)\/blob\/([^/]+)\/(.+)$/i)
  if (!m) return url
  return `https://raw.githubusercontent.com/${m[1]}/${m[2]}/${m[3]}/${m[4]}`
}

function guessAuditorFromUrl(u) {
  const s = String(u || '').toLowerCase()
  const patterns = [
    ['trail of bits', /trailofbits|trail[-_ ]of[-_ ]bits|tob/i],
    ['Zellic', /zellic/i],
    ['OpenZeppelin', /openzeppelin|oz/i],
    ['Spearbit', /spearbit/i],
    ['Cyfrin', /cyfrin/i],
    ['PeckShield', /peckshield/i],
    ['Halborn', /halborn/i],
    ['Certora', /certora/i],
    ['MixBytes', /mixbytes/i],
    ['Cantina', /cantina/i],
    ['Sherlock', /sherlock/i],
    ['Code4rena', /code4rena|code\\-423n4|c4/i],
    ['Quantstamp', /quantstamp/i],
    ['CertiK', /certik/i],
    ['Hacken', /hacken/i],
    ['Least Authority', /leastauthority/i],
    ['ConsenSys Diligence', /consensys.*diligence|diligence/i],
    ['OtterSec', /osec|ottersec/i],
  ]
  for (const [name, re] of patterns) if (re.test(s)) return name
  return ''
}

function titleFromUrl(u) {
  const s = String(u || '')
  try {
    const p = new URL(s).pathname
    const base = path.basename(p)
    const noExt = base.replace(/\.pdf$/i, '')
    return noExt.replace(/[_\-]+/g, ' ').replace(/\s+/g, ' ').trim() || base
  } catch {
    return s.slice(0, 80)
  }
}

function containsAlias(textNorm, alias) {
  const a = stripCommonSuffixes(alias)
  if (!a) return false
  const hay = ` ${textNorm} `
  const needle = ` ${a} `
  if (hay.includes(needle)) return true
  // Allow hyphen/underscore variants to match after normalization.
  return false
}

function flattenFirmIndex(firmIndex) {
  const out = []
  if (!firmIndex?.firms) return out
  for (const [firmSlug, firm] of Object.entries(firmIndex.firms)) {
    const firmName = firm?.name || firmSlug
    for (const r of firm.reports || []) {
      const nameExtractedNorm = stripCommonSuffixes(r.name_extracted || '')
      out.push({
        firm_slug: firmSlug,
        firm_name: firmName,
        source_type: firm.source_type || 'unknown',
        source_url: firm.source_url || '',
        report: r,
        name_extracted_norm: nameExtractedNorm,
        search_text: norm(
          [
            r.filename,
            r.title,
            r.repo_name,
            r.url,
            r.pdf_url,
            r.name_extracted,
            r.date_extracted,
          ].filter(Boolean).join(' ')
        ),
      })
    }
  }
  return out
}

function buildProtocolAliases(proto) {
  const out = new Set()
  out.add(proto.slug)
  out.add(stripCommonSuffixes(proto.name))
  if (ALIASES[proto.slug]) for (const a of ALIASES[proto.slug]) out.add(stripCommonSuffixes(a))
  for (const a of Array.from(out)) {
    if (!a) out.delete(a)
  }
  return Array.from(out).filter(Boolean)
}

function dedupeAudits(audits) {
  // Dedup by (auditor, date, pdf_url) with fallbacks; merge found_via/pdf_url.
  const byKey = new Map()
  for (const a of audits) {
    const key = [
      norm(a.auditor),
      a.date || '',
      norm(a.pdf_url || a.source_url || ''),
      stripCommonSuffixes(a.title || ''),
    ].join('|')
    const prev = byKey.get(key)
    if (!prev) {
      byKey.set(key, { ...a, found_via: Array.from(new Set(a.found_via || [a.source])) })
      continue
    }
    const merged = { ...prev }
    merged.found_via = Array.from(new Set([...(prev.found_via || []), ...(a.found_via || [a.source])]))
    if (!merged.pdf_url && a.pdf_url) merged.pdf_url = a.pdf_url
    if ((!merged.source_url || merged.source_url.length < 5) && a.source_url) merged.source_url = a.source_url
    if ((merged.title || '').length < (a.title || '').length) merged.title = a.title
    if (merged.confidence !== 'high' && a.confidence === 'high') merged.confidence = 'high'
    byKey.set(key, merged)
  }
  return Array.from(byKey.values())
}

async function scanGithubRepoForPdfs(githubUrl) {
  const or = parseGithubOwnerRepo(githubUrl)
  if (!or) return []

  // Fetch default branch (one call) then tree (one call). Much cheaper than Contents API loops per path.
  const repoRes = await githubFetch(`${GITHUB_API}/repos/${or.owner}/${or.repo}`)
  if (!repoRes.ok) return []
  const repoInfo = await repoRes.json()
  const ref = repoInfo.default_branch || 'main'

  const treeRes = await githubFetch(`${GITHUB_API}/repos/${or.owner}/${or.repo}/git/trees/${encodeURIComponent(ref)}?recursive=1`)
  if (!treeRes.ok) return []
  const tree = await treeRes.json()
  const items = Array.isArray(tree.tree) ? tree.tree : []

  const pdfs = []
  for (const it of items) {
    if (it.type !== 'blob') continue
    const p = String(it.path || '')
    if (!/\.pdf$/i.test(p)) continue
    const pNorm = p.toLowerCase()
    if (!AUDIT_DIR_PREFIXES.some(pref => pNorm.startsWith(pref))) continue
    const rawUrl = `https://raw.githubusercontent.com/${or.owner}/${or.repo}/${ref}/${p}`
    pdfs.push({ pdf_url: rawUrl, source_url: `https://github.com/${or.owner}/${or.repo}/blob/${ref}/${p}` })
  }
  return pdfs
}

function extractAnchorLinks(html) {
  const links = []
  const re = /<a\b[^>]*href\s*=\s*["']([^"']+)["'][^>]*>([\s\S]*?)<\/a>/gi
  let m
  while ((m = re.exec(html))) {
    const href = m[1]
    const text = (m[2] || '').replace(/<[^>]+>/g, ' ').replace(/\s+/g, ' ').trim()
    links.push({ href, text })
  }
  return links
}

function absolutize(base, href) {
  try {
    return new URL(href, base).toString()
  } catch {
    return href
  }
}

async function scanWebsiteForAuditLinks(website) {
  const urls = []
  if (!website) return urls
  const base = website.startsWith('http') ? website : `https://${website}`
  const candidates = [
    `${base.replace(/\/$/, '')}/security`,
    `${base.replace(/\/$/, '')}/audits`,
    `${base.replace(/\/$/, '')}/security/audits`,
    `${base.replace(/\/$/, '')}/docs/security`,
    `${base.replace(/\/$/, '')}/bug-bounty`,
  ]
  const found = []
  for (const u of candidates) {
    try {
      const res = await politeWebsiteFetch(u)
      if (!res.ok) continue
      const html = await res.text()
      const anchors = extractAnchorLinks(html)
      for (const a of anchors) {
        const abs = absolutize(u, a.href)
        if (/\.pdf(\?|#|$)/i.test(abs) || /\baudits?\b|\bsecurity\b/i.test(a.text || '') || /\/audit|\/audits|security/i.test(abs)) {
          found.push({ url: abs, text: a.text, page: u })
        }
      }
    } catch {
      // ignore
    }
  }
  for (const f of found) {
    urls.push({
      url: f.url,
      page: f.page,
      title: f.text || titleFromUrl(f.url),
    })
  }
  return urls
}

async function main() {
  const args = parseArgs(process.argv.slice(2))

  const merged = readJson(MERGED_ENRICHMENT_PATH, {})
  const websiteContacts = readJson(WEBSITE_CONTACTS_PATH, {})
  const firmIndex = fs.existsSync(FIRM_INDEX_PATH) ? readJson(FIRM_INDEX_PATH, null) : null
  const flatReports = flattenFirmIndex(firmIndex)

  const protocolFiles = fs
    .readdirSync(INPUT_PROTOCOL_DIR)
    .filter(f => f.endsWith('.json') && !f.startsWith('_'))
    .sort()

  const checkpoint = args.resume ? readJson(CHECKPOINT_PATH, null) : null
  const existing = args.resume && fs.existsSync(OUT_PATH) ? readJson(OUT_PATH, null) : null

  const output = existing || {
    metadata: {
      generated_at: new Date().toISOString(),
      protocols_processed: 0,
      protocols_with_audits: 0,
      total_audits_found: 0,
      sources_breakdown: {
        github_protocol: 0,
        protocol_website: 0,
        firm_index_match: 0,
        sherlock: 0,
        code4rena: 0,
        cantina: 0,
      },
    },
    protocols: {},
  }

  let startIndex = Math.max(0, args.start || 0)
  if (args.resume && checkpoint && Number.isFinite(checkpoint.last_index)) {
    startIndex = Math.max(startIndex, checkpoint.last_index + 1)
  }

  const endIndex = args.limit > 0 ? Math.min(protocolFiles.length, startIndex + args.limit) : protocolFiles.length

  for (let idx = startIndex; idx < endIndex; idx++) {
    const filename = protocolFiles[idx]
    const slug = filename.replace(/\.json$/i, '')
    const protocolJson = readJson(path.join(INPUT_PROTOCOL_DIR, filename), null) || {}

    const proto = {
      slug,
      name: protocolJson.name || slug,
      category: protocolJson.category || protocolJson?.metadata?.category || 'DeFi',
      chains: protocolJson.chains || protocolJson?.metadata?.chains || protocolJson?.metadata?.scope?.chains || [],
      github_url: merged?.[slug]?.github_url || websiteContacts?.[slug]?.github || '',
      website: merged?.[slug]?.website || websiteContacts?.[slug]?.website || '',
      docs_url: merged?.[slug]?.docs_url || websiteContacts?.[slug]?.docs_url || '',
    }

    const audits = []

    // Source: already-scraped audit links in merged enrichment
    for (const u of merged?.[slug]?.audit_report_urls || []) {
      const raw = toRawGithubUrl(u)
      const auditor = guessAuditorFromUrl(u) || guessAuditorFromUrl(raw) || 'External'
      const date = extractDate(u) || extractDate(raw)
      const title = titleFromUrl(u)
      audits.push({
        id: `${slug}-${safeIdPart(auditor)}-${safeIdPart(date || title)}`,
        title,
        auditor,
        date,
        source: /github\.com/i.test(u) ? 'github_protocol' : 'protocol_website',
        source_url: u,
        pdf_url: /\.pdf(\?|#|$)/i.test(raw) ? raw : '',
        category: proto.category,
        chains: proto.chains,
        primitive: classifyPrimitive(proto, title),
        confidence: 'high',
        found_via: ['merged_enrichment'],
      })
    }

    // Optional: live GitHub scanning for PDFs under audit/security folders
    if (args.scanGithub && proto.github_url) {
      try {
        const pdfs = await scanGithubRepoForPdfs(proto.github_url)
        for (const p of pdfs) {
          const auditor = guessAuditorFromUrl(p.source_url) || 'External'
          const date = extractDate(p.source_url) || extractDate(p.pdf_url)
          const title = titleFromUrl(p.source_url)
          audits.push({
            id: `${slug}-${safeIdPart(auditor)}-${safeIdPart(date || title)}`,
            title,
            auditor,
            date,
            source: 'github_protocol',
            source_url: p.source_url,
            pdf_url: p.pdf_url,
            category: proto.category,
            chains: proto.chains,
            primitive: classifyPrimitive(proto, title),
            confidence: 'high',
            found_via: ['github_tree_scan'],
          })
        }
      } catch {
        // ignore
      }
    }

    // Optional: live website scanning for audit/security pages
    if (args.scanWebsite && proto.website) {
      try {
        const links = await scanWebsiteForAuditLinks(proto.website)
        for (const l of links) {
          const auditor = guessAuditorFromUrl(l.url) || 'External'
          const date = extractDate(l.url) || extractDate(l.title)
          audits.push({
            id: `${slug}-${safeIdPart(auditor)}-${safeIdPart(date || l.title)}`,
            title: l.title,
            auditor,
            date,
            source: 'protocol_website',
            source_url: l.page || proto.website,
            pdf_url: /\.pdf(\?|#|$)/i.test(l.url) ? l.url : '',
            category: proto.category,
            chains: proto.chains,
            primitive: classifyPrimitive(proto, l.title),
            confidence: 'medium',
            found_via: ['website_scan'],
          })
        }
      } catch {
        // ignore
      }
    }

    // Source: match against firm index (local matching; no network)
    if (flatReports.length) {
      const aliases = buildProtocolAliases(proto)
      const aliasSet = new Set(aliases.map(a => stripCommonSuffixes(a)).filter(Boolean))
      // Avoid substring-false-positives for generic slugs/names unless the report provides explicit name_extracted.
      const generic = new Set(['foundation', 'native', 'icon', 'eco', 'gear', 'router', 'reserve', 'range', 'oasis', 'pact'])
      const orgStrict = new Set(['sherlock', 'code4rena'])
      for (const rep of flatReports) {
        const repName = rep.name_extracted_norm
        let matched = false
        let high = false

        if (orgStrict.has(rep.firm_slug)) {
          // Contest-org repos all live under URLs containing the org name; do NOT match on URL text.
          if (repName && aliasSet.has(repName)) {
            matched = true
            high = true
          } else {
            continue
          }
        } else if (repName && aliasSet.has(repName)) {
          matched = true
          high = true
        } else if (!generic.has(proto.slug)) {
          const text = rep.search_text
          // Prefer exact token-boundary matching on longer aliases to reduce accidental hits (e.g. "aster" in "master").
          for (const a of aliases) {
            const aNorm = stripCommonSuffixes(a)
            if (!aNorm) continue
            if (aNorm.length < 4 && aNorm !== proto.slug) continue
            if (containsAlias(text, aNorm)) {
              matched = true
              if (aNorm === stripCommonSuffixes(proto.slug) || aNorm === stripCommonSuffixes(proto.name)) high = true
              break
            }
          }
        }

        if (!matched) continue

        const r = rep.report || {}
        const pdfUrl = r.url && /\.pdf(\?|#|$)/i.test(r.url) ? r.url : r.pdf_url || r.url || ''
        const title = r.title || r.filename || r.repo_name || titleFromUrl(pdfUrl || r.url)
        const date = r.date_extracted || extractDate(title) || extractDate(pdfUrl)

        const source =
          rep.firm_slug === 'sherlock'
            ? 'sherlock'
            : rep.firm_slug === 'code4rena'
              ? 'code4rena'
              : rep.firm_slug === 'cantina'
                ? 'cantina'
                : 'firm_index_match'

        audits.push({
          id: `${slug}-${safeIdPart(rep.firm_name)}-${safeIdPart(date || title)}`,
          title,
          auditor: rep.firm_name,
          date,
          source,
          source_url: r.url || rep.source_url,
          pdf_url: pdfUrl,
          category: proto.category,
          chains: proto.chains,
          primitive: classifyPrimitive(proto, title),
          confidence: high ? 'high' : 'medium',
          found_via: ['firm_index'],
        })
      }
    }

    const finalAudits = dedupeAudits(audits)

    output.protocols[slug] = {
      slug,
      name: proto.name,
      audits: finalAudits,
    }

    // Update metadata counters
    output.metadata.protocols_processed = Object.keys(output.protocols).length
    output.metadata.protocols_with_audits = Object.values(output.protocols).filter(p => (p.audits || []).length).length
    output.metadata.total_audits_found = Object.values(output.protocols).reduce((n, p) => n + ((p.audits || []).length || 0), 0)

    const sb = output.metadata.sources_breakdown
    // Recompute breakdown from scratch (cheap at 458 protocols; avoids double-counting on resume)
    for (const k of Object.keys(sb)) sb[k] = 0
    for (const p of Object.values(output.protocols)) {
      for (const a of p.audits || []) {
        if (sb[a.source] !== undefined) sb[a.source]++
      }
    }

    output.metadata.generated_at = new Date().toISOString()
    writeJson(OUT_PATH, output)
    writeJson(CHECKPOINT_PATH, { last_index: idx, timestamp: new Date().toISOString() })

    console.log(`[${idx + 1}/${protocolFiles.length}] ${slug}: ${finalAudits.length} audits`)
  }

  console.log(`Wrote ${OUT_PATH}`)
}

main().catch(e => {
  console.error(e)
  process.exit(1)
})
